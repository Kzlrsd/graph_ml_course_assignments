{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Pc7WIxg9DF8",
        "outputId": "5c7403e0-69ab-461b-fd8b-793a6b09af6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://data.dgl.ai/wheels/repo.html\n",
            "Collecting dgl\n",
            "  Downloading dgl-1.1.1-cp310-cp310-manylinux1_x86_64.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.10.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.65.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.4)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-1.1.1\n"
          ]
        }
      ],
      "source": [
        "pip install  dgl -f https://data.dgl.ai/wheels/repo.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lk_YwUgk9J9l",
        "outputId": "dff7447a-53d2-457b-9c15-6505988dde30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://data.dgl.ai/wheels-test/repo.html\n",
            "Collecting dglgo\n",
            "  Downloading dglgo-0.0.2-py3-none-any.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dglgo) (0.7.0)\n",
            "Collecting isort>=5.10.1 (from dglgo)\n",
            "  Downloading isort-5.12.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autopep8>=1.6.0 (from dglgo)\n",
            "  Downloading autopep8-2.0.2-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpydoc>=1.1.0 (from dglgo)\n",
            "  Downloading numpydoc-1.5.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.4/52.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from dglgo) (1.10.9)\n",
            "Collecting ruamel.yaml>=0.17.20 (from dglgo)\n",
            "  Downloading ruamel.yaml-0.17.32-py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from dglgo) (6.0)\n",
            "Collecting ogb>=1.3.3 (from dglgo)\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rdkit-pypi (from dglgo)\n",
            "  Downloading rdkit_pypi-2022.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from dglgo) (1.2.2)\n",
            "Collecting pycodestyle>=2.10.0 (from autopep8>=1.6.0->dglgo)\n",
            "  Downloading pycodestyle-2.10.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from autopep8>=1.6.0->dglgo) (2.0.1)\n",
            "Collecting sphinx>=4.2 (from numpydoc>=1.1.0->dglgo)\n",
            "  Downloading sphinx-7.0.1-py3-none-any.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.10/dist-packages (from numpydoc>=1.1.0->dglgo) (3.1.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (4.65.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (1.26.16)\n",
            "Collecting outdated>=0.2.0 (from ogb>=1.3.3->dglgo)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.0->dglgo) (4.6.3)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.20->dglgo)\n",
            "  Downloading ruamel.yaml.clib-0.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (485 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->dglgo) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->dglgo) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->dglgo) (3.1.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.4.0->dglgo) (8.1.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi->dglgo) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.10->numpydoc>=1.1.0->dglgo) (2.1.3)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb>=1.3.3->dglgo) (67.7.2)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb>=1.3.3->dglgo)\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb>=1.3.3->dglgo) (2.27.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb>=1.3.3->dglgo) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb>=1.3.3->dglgo) (2022.7.1)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.0.4)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.0.2)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.1.5)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.0.3)\n",
            "Requirement already satisfied: Pygments>=2.13 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.14.0)\n",
            "Collecting docutils<0.21,>=0.18.1 (from sphinx>=4.2->numpydoc>=1.1.0->dglgo)\n",
            "  Downloading docutils-0.20.1-py3-none-any.whl (572 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.7/572.7 kB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: snowballstemmer>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.9 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.12.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (0.7.13)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.4.1)\n",
            "Requirement already satisfied: packaging>=21.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb>=1.3.3->dglgo) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb>=1.3.3->dglgo) (16.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb>=1.3.3->dglgo) (1.3.0)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7029 sha256=821e868a4ca00a305a5edbf235ed46644cfaa90a2e7aa70958e741b4bf4b89c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, ruamel.yaml.clib, rdkit-pypi, pycodestyle, isort, docutils, sphinx, ruamel.yaml, outdated, autopep8, numpydoc, ogb, dglgo\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.16\n",
            "    Uninstalling docutils-0.16:\n",
            "      Successfully uninstalled docutils-0.16\n",
            "  Attempting uninstall: sphinx\n",
            "    Found existing installation: Sphinx 3.5.4\n",
            "    Uninstalling Sphinx-3.5.4:\n",
            "      Successfully uninstalled Sphinx-3.5.4\n",
            "Successfully installed autopep8-2.0.2 dglgo-0.0.2 docutils-0.20.1 isort-5.12.0 littleutils-0.2.2 numpydoc-1.5.0 ogb-1.3.6 outdated-0.2.2 pycodestyle-2.10.0 rdkit-pypi-2022.9.5 ruamel.yaml-0.17.32 ruamel.yaml.clib-0.2.7 sphinx-7.0.1\n"
          ]
        }
      ],
      "source": [
        "pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Bex0KKXv9U8G"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import os\n",
        "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
        "import dgl\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import dgl.function as fn\n",
        "import torch.nn.functional as F\n",
        "import shutil\n",
        "from torch.utils.data import DataLoader\n",
        "import cloudpickle\n",
        "from dgl.nn import GraphConv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOi_tjP49g8G",
        "outputId": "b514f88f-07f5-4301-c65f-44df7b4a2566"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0sO4na7C9xW_"
      },
      "outputs": [],
      "source": [
        "current_dir = \"/content/gdrive/MyDrive/graph_data.zip\"\n",
        "#This variable (checkpoint_path) represents the path to the directory where model checkpoints will be saved.\n",
        "checkpoint_path = current_dir + \"save_models/model_checkpoints/\" + \"checkpoint\"\n",
        "#Creates the directory specified by checkpoint_path using os.makedirs(). If the directory already exists, it will not raise an error (exist_ok=True).\n",
        "os.makedirs(checkpoint_path, exist_ok=True)\n",
        "best_model_path = current_dir + \"save_models/best_model/\"\n",
        "folder_data_temp = current_dir +\"data_temp/\"\n",
        "#Uses shutil.rmtree() to delete the directory specified by folder_data_temp, including all its contents. The ignore_errors=True parameter ensures that errors encountered during the deletion process are ignored\n",
        "shutil.rmtree(folder_data_temp, ignore_errors=True)\n",
        "path_save = current_dir\n",
        "#Uses shutil.unpack_archive() to extract the contents of the archive specified by path_save into the directory specified by folder_data_temp. This line effectively unzips the \"graph_data.zip\" file into the \"data_temp\" directory.\n",
        "shutil.unpack_archive(path_save, folder_data_temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mM-tfB7C9-uw"
      },
      "outputs": [],
      "source": [
        "\"\"\" Classification Dataset \"\"\"\n",
        "class DGLDatasetClass(torch.utils.data.Dataset):\n",
        "    #This is the constructor method of the class. It takes a single argument address, which represents the path to the binary file containing the graph data.\n",
        "    def __init__(self, address):\n",
        "       # It sets the instance variable address to the provided address, but with \".bin\" appended at the end. This suggests that the graph data is stored in a binary file format.\n",
        "            self.address=address+\".bin\"\n",
        "            #It loads the DGL graphs from the binary file specified by address using the dgl.load_graphs() function. The resulting graphs are stored in self.list_graphs, and other data related to classification tasks, such as labels, masks, and globals, are stored in train_labels_masks_globals.\n",
        "            self.list_graphs, train_labels_masks_globals = dgl.load_graphs(self.address)\n",
        "            num_graphs =len(self.list_graphs)\n",
        "            \n",
        "         #It extracts the labels from the train_labels_masks_globals dictionary and reshapes them to have a size of (num_graphs, -1), where num_graphs is the number of graphs \n",
        "         # -1 indicates that the second dimension can have any size.\n",
        "            self.labels = train_labels_masks_globals[\"labels\"].view(num_graphs,-1)\n",
        "            self.masks = train_labels_masks_globals[\"masks\"].view(num_graphs,-1)\n",
        "            self.globals = train_labels_masks_globals[\"globals\"].view(num_graphs,-1)\n",
        "    def __len__(self):\n",
        "        return len(self.list_graphs)\n",
        "       #This method is used to retrieve an item from the dataset at a specific index idx. In this case, it returns the graph, labels, masks, and globals corresponding to the given index.\n",
        "    def __getitem__(self, idx):\n",
        "        return  self.list_graphs[idx], self.labels[idx], self.masks[idx], self.globals[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5T6PMShm-D4m",
        "outputId": "d6e39657-af72-4f90-fef4-60dd8cd0a843"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1631 203 205\n"
          ]
        }
      ],
      "source": [
        "#It constructs the path_data_temp by concatenating the folder_data_temp path (which represents the temporary data folder) with the string \"scaffold_0\". The resulting path_data_temp will be used to specify the address for loading the graph data.\n",
        "path_data_temp = folder_data_temp + \"scaffold\"+\"_\"+str(0)\n",
        "#It creates an instance of the DGLDatasetClass for the training set by providing the address as path_data_temp+\"_train\". This implies that the training set graph data is stored in a binary file with the suffix \"_train\" appended to the path_data_temp.\n",
        "train_set = DGLDatasetClass(address=path_data_temp+\"_train\")\n",
        "\"): Similarly, it creates an instance of the DGLDatasetClass for the validation set, assuming the graph data is stored in a binary file with the suffix \"_val\" appended to the path_data_temp.\n",
        "val_set = DGLDatasetClass(address=path_data_temp+\"_val\")\n",
        "test_set = DGLDatasetClass(address=path_data_temp+\"_test\")\n",
        "\n",
        "print(len(train_set), len(val_set), len(test_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-eNx3vzQ-Jhe"
      },
      "outputs": [],
      "source": [
        "def collate(batch):\n",
        "    #This function takes a batch of samples, where each sample is a tuple (graphs, labels, masks, globals). \n",
        "    # batch is a list of tuples (graphs, labels, masks, globals)\n",
        "    # Extracts the graphs from each sample in the batch and creates a batched graph using dgl.batch(). The resulting batched graph is stored in the variable g.\n",
        "    graphs = [e[0] for e in batch]\n",
        "    g = dgl.batch(graphs)\n",
        "\n",
        "    # Extracts the labels from each sample in the batch and stacks them along a new dimension using torch.stack(). The resulting tensor is stored in the variable labels.\n",
        "    labels = [e[1] for e in batch]\n",
        "    labels = torch.stack(labels, 0)\n",
        "\n",
        "    # Concatenate a sequence of tensors (masks) along a new dimension\n",
        "    masks = [e[2] for e in batch]\n",
        "    masks = torch.stack(masks, 0)\n",
        "\n",
        "    # Concatenate a sequence of tensors (globals) along a new dimension\n",
        "    globals = [e[3] for e in batch]\n",
        "    globals = torch.stack(globals, 0)\n",
        "\n",
        "    return g, labels, masks, globals\n",
        "\n",
        "#This function creates and returns three data loaders: train_dataloader, val_dataloader, and test_dataloader\n",
        "def loader(batch_size=64):\n",
        "    train_dataloader = DataLoader(train_set,\n",
        "                              batch_size=batch_size,\n",
        "                              collate_fn=collate,\n",
        "                              drop_last=False,\n",
        "                              shuffle=True,\n",
        "                              num_workers=1)\n",
        "\n",
        "    val_dataloader =  DataLoader(val_set,\n",
        "                             batch_size=batch_size,\n",
        "                             collate_fn=collate,\n",
        "                             drop_last=False,\n",
        "                             shuffle=False,\n",
        "                             num_workers=1)\n",
        "\n",
        "    test_dataloader = DataLoader(test_set,\n",
        "                             batch_size=batch_size,\n",
        "                             collate_fn=collate,\n",
        "                             drop_last=False,\n",
        "                             shuffle=False,\n",
        "                             num_workers=1)\n",
        "    return train_dataloader, val_dataloader, test_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "J8qhBdol-QPt"
      },
      "outputs": [],
      "source": [
        "train_dataloader, val_dataloader, test_dataloader = loader(batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SEPHb_zs-awh"
      },
      "outputs": [],
      "source": [
        "#BBBP dataset has 1 task.\n",
        "num_tasks = 1\n",
        "\n",
        "# Size of global feature of each graph\n",
        "global_size = 200\n",
        "\n",
        "# Number of epochs to train the model\n",
        "num_epochs = 100\n",
        "\n",
        "# Number of steps to wait if the model performance on the validation set does not improve\n",
        "patience = 10\n",
        "\n",
        "#Configurations to instantiate the model\n",
        "config = {\"node_feature_size\":127, \"edge_feature_size\":12, \"hidden_size\":100}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "1RglNsKV-cie"
      },
      "outputs": [],
      "source": [
        "#defines a GNN (Graph Neural Network) model as a subclass of nn.Module in PyTorch\n",
        "class GNN(nn.Module):\n",
        "    #This is the constructor method of the GNN class. It takes three parameters: #The configuration dictionary used to instantiate the model\n",
        "    def __init__(self, config, global_size = 200, num_tasks = 1):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_tasks = num_tasks\n",
        "\n",
        "        # Node feature size\n",
        "        self.node_feature_size = self.config.get('node_feature_size', 127)\n",
        "\n",
        "        # Edge feature size\n",
        "        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n",
        "\n",
        "        # Hidden size\n",
        "        self.hidden_size = self.config.get('hidden_size', 100)\n",
        "\n",
        "    # This line creates an instance of the GraphConv layer from the DGL library. It takes the node feature size, hidden size, and an optional parameter allow_zero_in_degree as arguments. The GraphConv layer performs graph convolutional operations on the input graph\n",
        "        self.conv1 = GraphConv(self.node_feature_size, self.hidden_size,allow_zero_in_degree=True)\n",
        "        self.conv2 = GraphConv(self.hidden_size, self.num_tasks,allow_zero_in_degree=True)\n",
        "\n",
        "    #This method defines the forward pass of the model. It takes two arguments:\n",
        "    # mol_dgl_graph: The DGL graph object representing the molecular graph. #globals: The global features for the graph.\n",
        "    def forward(self, mol_dgl_graph, globals):\n",
        "        #In the forward method, the node and edge features of the input graph are sliced to match the specified feature sizes. This is done using mol_dgl_graph.ndata[\"v\"] and mol_dgl_graph.edata[\"e\"] respectively.\n",
        "        mol_dgl_graph.ndata[\"v\"]= mol_dgl_graph.ndata[\"v\"][:,:self.node_feature_size]\n",
        "        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:,:self.edge_feature_size]\n",
        "        #The input graph is then passed through the first GraphConv layer (self.conv1) with a ReLU activation function applied to the hidden features (h). The result is stored in the variable h.\n",
        "        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"])\n",
        "        h = F.relu(h)\n",
        "        #8.\tNext, h is passed through the second GraphConv layer (self.conv2) to obtain the final node representations for each task.\n",
        "        h = self.conv2(mol_dgl_graph, h)\n",
        "        #9.\tFinally, the node representations are averaged using dgl.mean_nodes() over the entire graph and returned as the output of the forward pass.\n",
        "        mol_dgl_graph.ndata[\"h\"] = h\n",
        "        return dgl.mean_nodes(mol_dgl_graph, \"h\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6-9e4LWF-ju-"
      },
      "outputs": [],
      "source": [
        "#It will be used as the evaluation metric for computing the model's performance.\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def compute_score(model, data_loader, val_size, num_tasks):\n",
        "    #The function starts by setting the model to evaluation mode using model.eval().\n",
        "    model.eval()\n",
        "    #The metric variable is set to roc_auc_score, indicating that the ROC AUC score will be used as the evaluation metric. \n",
        "    metric = roc_auc_score\n",
        "    with torch.no_grad():\n",
        "        #The function initializes empty tensors (prediction_all, labels_all, masks_all) to store the predictions, labels, and masks for all samples in the dataset.\n",
        "        prediction_all= torch.empty(0)\n",
        "        labels_all= torch.empty(0)\n",
        "        masks_all= torch.empty(0)\n",
        "        #The function iterates over the data loader, which provides batches of samples. For each batch, the model is applied to the input graph and global features to obtain the predictions. The predictions are then converted to probabilities using the sigmoid function (torch.sigmoid(prediction)).\n",
        "        for i, (mol_dgl_graph, labels, masks, globals) in enumerate(data_loader):\n",
        "            prediction = model(mol_dgl_graph, globals)\n",
        "            prediction = torch.sigmoid(prediction)\n",
        "            #The batch predictions, labels, and masks are concatenated with the existing tensors using torch.cat().\n",
        "            prediction_all = torch.cat((prediction_all, prediction), 0)\n",
        "            labels_all = torch.cat((labels_all, labels), 0)\n",
        "            masks_all = torch.cat((masks_all, masks), 0)\n",
        "            #For each task, the function extracts the relevant predictions, labels, and masks and calculates the metric score (ROC AUC score) using roc_auc_score. If a ValueError \n",
        "            # occurs during the calculation, indicating that the task has no positive or negative samples, the score is set to 0.\n",
        "        #The calculated score is added to the average tensor.\n",
        "        average = torch.tensor([0.])\n",
        "        for i in range(num_tasks):\n",
        "            a1 = prediction_all[:, i][masks_all[:,i]==1]\n",
        "            a2 = labels_all[:, i][masks_all[:,i]==1]\n",
        "            try:\n",
        "                t = metric(a2.int().cpu(), a1.cpu()).item()\n",
        "            except ValueError:\n",
        "                t = 0\n",
        "            average += t\n",
        "   #Finally, the function returns the average score, which is the sum of individual task scores divided by the number of tasks.\n",
        "    return average.item()/num_tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5pflRe46-oH0"
      },
      "outputs": [],
      "source": [
        "def loss_func(output, label, mask, num_tasks):\n",
        "    # This line creates a tensor pos_weight with ones, representing the positive weights for each task. The tensor has a shape of (1, num_tasks), where num_tasks is the number of tasks for the model.\n",
        "    pos_weight = torch.ones((1, num_tasks))\n",
        "    pos_weight\n",
        "    # This line creates an instance of the BCEWithLogitsLoss class from PyTorch's torch.nn module\n",
        "    criterion = torch.nn.BCEWithLogitsLoss(reduction='none', pos_weight=pos_weight)\n",
        "    #This line creates an instance of the BCEWithLogitsLoss class from PyTorch's torch.nn module. \n",
        "    loss = mask*criterion(output,label)\n",
        "    #This line calculates the average loss for the batch by summing the losses and dividing by the sum of the mask values. This ensures that the loss is normalized based on the number of valid samples.\n",
        "    loss = loss.sum() / mask.sum()\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Vp6rCGEK-tuP"
      },
      "outputs": [],
      "source": [
        "#Initializes the epoch_train_loss variable to keep track of the average training loss for the epoch.\n",
        "def train_epoch(train_dataloader, model, optimizer):\n",
        "    epoch_train_loss = 0\n",
        "    iterations = 0\n",
        "    #Sets the model in training mode using model.train(). This is necessary to enable training-specific behavior such as dropout\n",
        "    model.train() # Prepare model for training\n",
        "    for i, (mol_dgl_graph, labels, masks, globals) in enumerate(train_dataloader): \n",
        "        #For each batch: a. Passes the molecular DGL graph (mol_dgl_graph) and global features (globals) through the model to obtain predictions. b. Calculates the training loss (loss_train) by comparing the predictions with the true labels (labels) using the provided loss_func. c. Zeros out the gradients of the optimizer using optimizer.zero_grad(set_to_none=True) to prevent accumulation of gradients from previous iterations. d. Performs backpropagation by calling loss_train.backward() to compute the gradients of the loss with respect to the model parameters. e. Updates the model parameters using the gradients by calling optimizer.step(). f. Adds the current batch's loss to epoch_train_loss. g. Increments the iterations counter.\n",
        "        prediction = model(mol_dgl_graph, globals)\n",
        "        loss_train = loss_func(prediction, labels, masks, num_tasks)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "        epoch_train_loss += loss_train.detach().item()\n",
        "        #Divides epoch_train_loss by the total number of iterations to obtain the average training loss for the epoch.\n",
        "        iterations += 1\n",
        "    epoch_train_loss /= iterations\n",
        "    return epoch_train_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "8Tnt0Dmk-0GK"
      },
      "outputs": [],
      "source": [
        "def train_evaluate():\n",
        "    #Initializes the GNN model (model) and the optimizer using the Adam optimizer with a learning rate of 0.0001.\n",
        "    model = GNN(config, global_size, num_tasks)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
        "#Sets the initial values for variables best_val, patience_count, and epoch. best_val keeps track of the best validation score obtained so far, patience_count counts the number of epochs since the last improvement in validation score, and epoch represents the current epoch number.\n",
        "    best_val = 0\n",
        "    patience_count = 1\n",
        "    epoch = 1\n",
        "#Starts a while loop that continues until the epoch reaches the maximum number of epochs (num_epochs).\n",
        "    while epoch <= num_epochs:\n",
        "        #Within the loop, checks if the patience_count is within the allowed patience limit. If it is, proceeds with training and evaluation.\n",
        "        if patience_count <= patience:\n",
        "        #Sets the model in training mode (model.train()) and calls the train_epoch function to perform one epoch of training on the training data. The training loss (loss_train) is returned\n",
        "            model.train()\n",
        "            loss_train = train_epoch(train_dataloader, model, optimizer)\n",
        "            #6.\tSets the model in evaluation mode (model.eval()) and calls the compute_score function to calculate the score on the validation set (val_dataloader). The score_val represents the evaluation score.\n",
        "            model.eval()\n",
        "            #7.\tCompares the score_val with the best_val. If it is higher, updates best_val with the new score and saves the model checkpoint.\n",
        "            score_val = compute_score(model, val_dataloader, len(val_set), num_tasks)\n",
        "            if score_val > best_val:\n",
        "                best_val = score_val\n",
        "                print(\"Save checkpoint\")\n",
        "                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n",
        "                dict_checkpoint = {\"score_val\": score_val}\n",
        "                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n",
        "                with open(path, \"wb\") as outputfile:\n",
        "                    cloudpickle.dump(dict_checkpoint, outputfile)\n",
        "                patience_count = 1\n",
        "            else:\n",
        "                print(\"Patience\", patience_count)\n",
        "                patience_count += 1\n",
        "\n",
        "            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n",
        "            epoch, num_epochs, loss_train, score_val))\n",
        "\n",
        "            print(\" \")\n",
        "            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n",
        "        epoch += 1\n",
        "\n",
        "    # best model save\n",
        "    shutil.rmtree(best_model_path, ignore_errors=True)\n",
        "    shutil.copytree(checkpoint_path, best_model_path)\n",
        "\n",
        "    print(\"Final results:\")\n",
        "    print(\"Average Valid Score: {:.3f}\".format(np.mean(best_val)), \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0dZa1mts-8tM"
      },
      "outputs": [],
      "source": [
        "#this function trains a GNN model for a specified number of epochs, tracks the best validation score, and saves the best model checkpoint. It also prints the training progress and final results\n",
        "def test_evaluate():\n",
        "    final_model = GNN(config, global_size, num_tasks)\n",
        "    path = os.path.join(best_model_path, 'checkpoint.pth')\n",
        "    with open(path, 'rb') as f:\n",
        "        checkpoint = cloudpickle.load(f)\n",
        "    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    final_model.eval()\n",
        "    test_score = compute_score(final_model, test_dataloader, len(test_set), num_tasks)\n",
        "\n",
        "    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n",
        "    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2WPGfhnAolz",
        "outputId": "561d6631-b76d-4521-ae7e-0a4ccf13b8b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Save checkpoint\n",
            "Epoch: 1/100 | Training Loss: 0.637 | Valid Score: 0.390\n",
            " \n",
            "Epoch: 1/100 | Best Valid Score Until Now: 0.390 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 2/100 | Training Loss: 0.616 | Valid Score: 0.361\n",
            " \n",
            "Epoch: 2/100 | Best Valid Score Until Now: 0.390 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 3/100 | Training Loss: 0.601 | Valid Score: 0.357\n",
            " \n",
            "Epoch: 3/100 | Best Valid Score Until Now: 0.390 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 4/100 | Training Loss: 0.591 | Valid Score: 0.366\n",
            " \n",
            "Epoch: 4/100 | Best Valid Score Until Now: 0.390 \n",
            "\n",
            "Patience 4\n",
            "Epoch: 5/100 | Training Loss: 0.583 | Valid Score: 0.373\n",
            " \n",
            "Epoch: 5/100 | Best Valid Score Until Now: 0.390 \n",
            "\n",
            "Patience 5\n",
            "Epoch: 6/100 | Training Loss: 0.580 | Valid Score: 0.387\n",
            " \n",
            "Epoch: 6/100 | Best Valid Score Until Now: 0.390 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 7/100 | Training Loss: 0.576 | Valid Score: 0.405\n",
            " \n",
            "Epoch: 7/100 | Best Valid Score Until Now: 0.405 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 8/100 | Training Loss: 0.571 | Valid Score: 0.432\n",
            " \n",
            "Epoch: 8/100 | Best Valid Score Until Now: 0.432 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 9/100 | Training Loss: 0.567 | Valid Score: 0.455\n",
            " \n",
            "Epoch: 9/100 | Best Valid Score Until Now: 0.455 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 10/100 | Training Loss: 0.564 | Valid Score: 0.489\n",
            " \n",
            "Epoch: 10/100 | Best Valid Score Until Now: 0.489 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 11/100 | Training Loss: 0.558 | Valid Score: 0.520\n",
            " \n",
            "Epoch: 11/100 | Best Valid Score Until Now: 0.520 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 12/100 | Training Loss: 0.558 | Valid Score: 0.553\n",
            " \n",
            "Epoch: 12/100 | Best Valid Score Until Now: 0.553 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 13/100 | Training Loss: 0.555 | Valid Score: 0.577\n",
            " \n",
            "Epoch: 13/100 | Best Valid Score Until Now: 0.577 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 14/100 | Training Loss: 0.549 | Valid Score: 0.595\n",
            " \n",
            "Epoch: 14/100 | Best Valid Score Until Now: 0.595 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 15/100 | Training Loss: 0.546 | Valid Score: 0.620\n",
            " \n",
            "Epoch: 15/100 | Best Valid Score Until Now: 0.620 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 16/100 | Training Loss: 0.543 | Valid Score: 0.633\n",
            " \n",
            "Epoch: 16/100 | Best Valid Score Until Now: 0.633 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 17/100 | Training Loss: 0.536 | Valid Score: 0.650\n",
            " \n",
            "Epoch: 17/100 | Best Valid Score Until Now: 0.650 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 18/100 | Training Loss: 0.536 | Valid Score: 0.667\n",
            " \n",
            "Epoch: 18/100 | Best Valid Score Until Now: 0.667 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 19/100 | Training Loss: 0.533 | Valid Score: 0.687\n",
            " \n",
            "Epoch: 19/100 | Best Valid Score Until Now: 0.687 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 20/100 | Training Loss: 0.529 | Valid Score: 0.701\n",
            " \n",
            "Epoch: 20/100 | Best Valid Score Until Now: 0.701 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 21/100 | Training Loss: 0.527 | Valid Score: 0.713\n",
            " \n",
            "Epoch: 21/100 | Best Valid Score Until Now: 0.713 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 22/100 | Training Loss: 0.522 | Valid Score: 0.725\n",
            " \n",
            "Epoch: 22/100 | Best Valid Score Until Now: 0.725 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 23/100 | Training Loss: 0.521 | Valid Score: 0.729\n",
            " \n",
            "Epoch: 23/100 | Best Valid Score Until Now: 0.729 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 24/100 | Training Loss: 0.517 | Valid Score: 0.740\n",
            " \n",
            "Epoch: 24/100 | Best Valid Score Until Now: 0.740 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 25/100 | Training Loss: 0.514 | Valid Score: 0.747\n",
            " \n",
            "Epoch: 25/100 | Best Valid Score Until Now: 0.747 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 26/100 | Training Loss: 0.511 | Valid Score: 0.754\n",
            " \n",
            "Epoch: 26/100 | Best Valid Score Until Now: 0.754 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 27/100 | Training Loss: 0.507 | Valid Score: 0.761\n",
            " \n",
            "Epoch: 27/100 | Best Valid Score Until Now: 0.761 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 28/100 | Training Loss: 0.506 | Valid Score: 0.761\n",
            " \n",
            "Epoch: 28/100 | Best Valid Score Until Now: 0.761 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 29/100 | Training Loss: 0.506 | Valid Score: 0.768\n",
            " \n",
            "Epoch: 29/100 | Best Valid Score Until Now: 0.768 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 30/100 | Training Loss: 0.502 | Valid Score: 0.775\n",
            " \n",
            "Epoch: 30/100 | Best Valid Score Until Now: 0.775 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 31/100 | Training Loss: 0.498 | Valid Score: 0.781\n",
            " \n",
            "Epoch: 31/100 | Best Valid Score Until Now: 0.781 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 32/100 | Training Loss: 0.495 | Valid Score: 0.781\n",
            " \n",
            "Epoch: 32/100 | Best Valid Score Until Now: 0.781 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 33/100 | Training Loss: 0.492 | Valid Score: 0.786\n",
            " \n",
            "Epoch: 33/100 | Best Valid Score Until Now: 0.786 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 34/100 | Training Loss: 0.489 | Valid Score: 0.792\n",
            " \n",
            "Epoch: 34/100 | Best Valid Score Until Now: 0.792 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 35/100 | Training Loss: 0.490 | Valid Score: 0.792\n",
            " \n",
            "Epoch: 35/100 | Best Valid Score Until Now: 0.792 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 36/100 | Training Loss: 0.486 | Valid Score: 0.797\n",
            " \n",
            "Epoch: 36/100 | Best Valid Score Until Now: 0.797 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 37/100 | Training Loss: 0.482 | Valid Score: 0.798\n",
            " \n",
            "Epoch: 37/100 | Best Valid Score Until Now: 0.798 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 38/100 | Training Loss: 0.482 | Valid Score: 0.799\n",
            " \n",
            "Epoch: 38/100 | Best Valid Score Until Now: 0.799 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 39/100 | Training Loss: 0.479 | Valid Score: 0.802\n",
            " \n",
            "Epoch: 39/100 | Best Valid Score Until Now: 0.802 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 40/100 | Training Loss: 0.478 | Valid Score: 0.803\n",
            " \n",
            "Epoch: 40/100 | Best Valid Score Until Now: 0.803 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 41/100 | Training Loss: 0.478 | Valid Score: 0.803\n",
            " \n",
            "Epoch: 41/100 | Best Valid Score Until Now: 0.803 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 42/100 | Training Loss: 0.472 | Valid Score: 0.804\n",
            " \n",
            "Epoch: 42/100 | Best Valid Score Until Now: 0.804 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 43/100 | Training Loss: 0.472 | Valid Score: 0.806\n",
            " \n",
            "Epoch: 43/100 | Best Valid Score Until Now: 0.806 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 44/100 | Training Loss: 0.469 | Valid Score: 0.806\n",
            " \n",
            "Epoch: 44/100 | Best Valid Score Until Now: 0.806 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 45/100 | Training Loss: 0.466 | Valid Score: 0.807\n",
            " \n",
            "Epoch: 45/100 | Best Valid Score Until Now: 0.807 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 46/100 | Training Loss: 0.466 | Valid Score: 0.807\n",
            " \n",
            "Epoch: 46/100 | Best Valid Score Until Now: 0.807 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 47/100 | Training Loss: 0.468 | Valid Score: 0.809\n",
            " \n",
            "Epoch: 47/100 | Best Valid Score Until Now: 0.809 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 48/100 | Training Loss: 0.465 | Valid Score: 0.810\n",
            " \n",
            "Epoch: 48/100 | Best Valid Score Until Now: 0.810 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 49/100 | Training Loss: 0.463 | Valid Score: 0.809\n",
            " \n",
            "Epoch: 49/100 | Best Valid Score Until Now: 0.810 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 50/100 | Training Loss: 0.461 | Valid Score: 0.810\n",
            " \n",
            "Epoch: 50/100 | Best Valid Score Until Now: 0.810 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 51/100 | Training Loss: 0.463 | Valid Score: 0.811\n",
            " \n",
            "Epoch: 51/100 | Best Valid Score Until Now: 0.811 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 52/100 | Training Loss: 0.458 | Valid Score: 0.812\n",
            " \n",
            "Epoch: 52/100 | Best Valid Score Until Now: 0.812 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 53/100 | Training Loss: 0.459 | Valid Score: 0.813\n",
            " \n",
            "Epoch: 53/100 | Best Valid Score Until Now: 0.813 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 54/100 | Training Loss: 0.457 | Valid Score: 0.813\n",
            " \n",
            "Epoch: 54/100 | Best Valid Score Until Now: 0.813 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 55/100 | Training Loss: 0.454 | Valid Score: 0.814\n",
            " \n",
            "Epoch: 55/100 | Best Valid Score Until Now: 0.814 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 56/100 | Training Loss: 0.456 | Valid Score: 0.813\n",
            " \n",
            "Epoch: 56/100 | Best Valid Score Until Now: 0.814 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 57/100 | Training Loss: 0.452 | Valid Score: 0.815\n",
            " \n",
            "Epoch: 57/100 | Best Valid Score Until Now: 0.815 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 58/100 | Training Loss: 0.454 | Valid Score: 0.815\n",
            " \n",
            "Epoch: 58/100 | Best Valid Score Until Now: 0.815 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 59/100 | Training Loss: 0.449 | Valid Score: 0.816\n",
            " \n",
            "Epoch: 59/100 | Best Valid Score Until Now: 0.816 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 60/100 | Training Loss: 0.448 | Valid Score: 0.817\n",
            " \n",
            "Epoch: 60/100 | Best Valid Score Until Now: 0.817 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 61/100 | Training Loss: 0.451 | Valid Score: 0.817\n",
            " \n",
            "Epoch: 61/100 | Best Valid Score Until Now: 0.817 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 62/100 | Training Loss: 0.447 | Valid Score: 0.818\n",
            " \n",
            "Epoch: 62/100 | Best Valid Score Until Now: 0.818 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 63/100 | Training Loss: 0.445 | Valid Score: 0.818\n",
            " \n",
            "Epoch: 63/100 | Best Valid Score Until Now: 0.818 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 64/100 | Training Loss: 0.449 | Valid Score: 0.819\n",
            " \n",
            "Epoch: 64/100 | Best Valid Score Until Now: 0.819 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 65/100 | Training Loss: 0.447 | Valid Score: 0.819\n",
            " \n",
            "Epoch: 65/100 | Best Valid Score Until Now: 0.819 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 66/100 | Training Loss: 0.446 | Valid Score: 0.820\n",
            " \n",
            "Epoch: 66/100 | Best Valid Score Until Now: 0.820 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 67/100 | Training Loss: 0.445 | Valid Score: 0.821\n",
            " \n",
            "Epoch: 67/100 | Best Valid Score Until Now: 0.821 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 68/100 | Training Loss: 0.446 | Valid Score: 0.821\n",
            " \n",
            "Epoch: 68/100 | Best Valid Score Until Now: 0.821 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 69/100 | Training Loss: 0.442 | Valid Score: 0.822\n",
            " \n",
            "Epoch: 69/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 70/100 | Training Loss: 0.441 | Valid Score: 0.821\n",
            " \n",
            "Epoch: 70/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 71/100 | Training Loss: 0.441 | Valid Score: 0.822\n",
            " \n",
            "Epoch: 71/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 72/100 | Training Loss: 0.442 | Valid Score: 0.821\n",
            " \n",
            "Epoch: 72/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 73/100 | Training Loss: 0.442 | Valid Score: 0.822\n",
            " \n",
            "Epoch: 73/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 74/100 | Training Loss: 0.442 | Valid Score: 0.822\n",
            " \n",
            "Epoch: 74/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 75/100 | Training Loss: 0.444 | Valid Score: 0.822\n",
            " \n",
            "Epoch: 75/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 76/100 | Training Loss: 0.440 | Valid Score: 0.822\n",
            " \n",
            "Epoch: 76/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Patience 4\n",
            "Epoch: 77/100 | Training Loss: 0.441 | Valid Score: 0.822\n",
            " \n",
            "Epoch: 77/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Patience 5\n",
            "Epoch: 78/100 | Training Loss: 0.437 | Valid Score: 0.821\n",
            " \n",
            "Epoch: 78/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Patience 6\n",
            "Epoch: 79/100 | Training Loss: 0.437 | Valid Score: 0.821\n",
            " \n",
            "Epoch: 79/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Patience 7\n",
            "Epoch: 80/100 | Training Loss: 0.436 | Valid Score: 0.822\n",
            " \n",
            "Epoch: 80/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Patience 8\n",
            "Epoch: 81/100 | Training Loss: 0.436 | Valid Score: 0.822\n",
            " \n",
            "Epoch: 81/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Patience 9\n",
            "Epoch: 82/100 | Training Loss: 0.436 | Valid Score: 0.822\n",
            " \n",
            "Epoch: 82/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Patience 10\n",
            "Epoch: 83/100 | Training Loss: 0.435 | Valid Score: 0.822\n",
            " \n",
            "Epoch: 83/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Final results:\n",
            "Average Valid Score: 0.822 \n",
            "\n",
            "Test Score: 0.620 \n",
            "\n",
            "Execution time: 49.983 seconds\n"
          ]
        }
      ],
      "source": [
        "#this function loads the best model checkpoint, initializes a new GNN model with the same configuration, evaluates its performance on the test dataset, and prints the test score as well as the execution time.\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "train_evaluate()\n",
        "test_evaluate()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
